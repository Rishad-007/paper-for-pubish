# Hybrid Economic Policy Analysis

A comprehensive framework combining LSTM neural networks, Double Machine Learning, and Causal Forest methodologies for robust economic policy impact assessment.

## ğŸ“– Overview

This project implements a novel hybrid approach to economic policy analysis by integrating three state-of-the-art machine learning methodologies:

- **LSTM Neural Networks** for macroeconomic time series forecasting
- **Double Machine Learning (DML)** for causal effect estimation
- **Causal Forest** for heterogeneous treatment effect analysis
- **Hybrid Ensemble** for integrated policy recommendations

The framework uses **real US economic data** from authoritative sources (FRED API, BLS, Business Dynamics Statistics) to analyze the causal impact of tax policy changes on firm survival rates.

## ğŸ¯ Key Features

### âœ… **Data Integrity**

- **100% Real Data**: No synthetic or artificial data used anywhere
- **Verified Sources**: FRED API, Bureau of Labor Statistics, Business Dynamics Statistics
- **Reproducible**: All random seeds set to 42 for full reproducibility

### ğŸ”¬ **Advanced Methodology**

- **Multi-Model Integration**: Combines forecasting, causal inference, and heterogeneity analysis
- **Robust Causal Inference**: Uses Double ML with cross-fitting to eliminate confounding bias
- **Heterogeneous Effects**: Discovers how policy impacts vary across economic conditions
- **Ensemble Weighting**: Data-driven combination of model predictions

### ğŸ“Š **Publication-Ready Outputs**

- **5 Summary Tables**: Descriptive statistics, model performance, policy impacts
- **3 High-Quality Figures**: Model architecture, data overview, results visualization
- **Executive Summary**: Comprehensive analysis summary in JSON and text formats
- **Academic Standards**: All outputs formatted for thesis publication

## ğŸ—ï¸ Project Structure

```
primary-thesis/
â”œâ”€â”€ HybridEconomicPolicyAnalysis.ipynb    # Main analysis notebook
â”œâ”€â”€ README.md                             # This file
â”œâ”€â”€ database/                             # Raw economic data files
â”‚   â”œâ”€â”€ bds2022.csv                      # Business Dynamics Statistics
â”‚   â”œâ”€â”€ CPIAUCSL.csv                     # Consumer Price Index
â”‚   â”œâ”€â”€ GDPC1.csv                        # Real GDP data
â”‚   â””â”€â”€ UNRATE.csv                       # Unemployment rate data
â”œâ”€â”€ data/                                # Processed datasets
â”œâ”€â”€ figures/                             # Generated visualizations
â”œâ”€â”€ models/                              # Trained model artifacts
â”œâ”€â”€ results/                             # Analysis results
â””â”€â”€ exports/                             # Publication-ready outputs
    â”œâ”€â”€ table1_descriptive_statistics.csv
    â”œâ”€â”€ table2_model_performance.csv
    â”œâ”€â”€ table3_policy_impact_summary.csv
    â”œâ”€â”€ table4_economic_forecasts.csv
    â”œâ”€â”€ table5_detailed_model_comparison.csv
    â”œâ”€â”€ figure1_model_architecture.png
    â”œâ”€â”€ figure2_economic_data_overview.png
    â”œâ”€â”€ figure3_hybrid_model_results.png
    â”œâ”€â”€ executive_summary.txt
    â”œâ”€â”€ executive_summary.json
    â””â”€â”€ results_package_manifest.json
```

## ğŸš€ Getting Started

### Prerequisites

```bash
# Core dependencies
pip install pandas numpy matplotlib seaborn
pip install tensorflow keras scikit-learn
pip install econml  # For causal inference
pip install fredapi  # For FRED data access
pip install requests beautifulsoup4  # For web scraping

# Jupyter environment
pip install jupyter ipykernel
```

### Quick Start

1. **Clone the repository**

   ```bash
   git clone <repository-url>
   cd primary-thesis
   ```

2. **Set up environment**

   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   pip install -r requirements.txt
   ```

3. **Run the analysis**

   ```bash
   jupyter notebook HybridEconomicPolicyAnalysis.ipynb
   ```

4. **Execute all cells** in order to reproduce the complete analysis

## ğŸ“‹ Methodology Details

### 1. **Data Pipeline**

- **FRED API Integration**: Automatic download of macroeconomic indicators
- **BLS Data Processing**: Unemployment and labor statistics
- **Business Dynamics**: Firm-level survival and creation rates
- **Feature Engineering**: Economic regimes, volatility measures, policy indicators

### 2. **LSTM Forecasting**

- **Architecture**: Multi-layer LSTM with dropout regularization
- **Features**: GDP growth, unemployment, inflation, interest rates
- **Validation**: Time series cross-validation with walk-forward analysis
- **Output**: 5-year economic forecasts with uncertainty quantification

### 3. **Double Machine Learning**

- **Estimator**: LinearDML with cross-fitting
- **Treatment**: Tax policy changes (cuts vs. increases)
- **Outcome**: Firm survival rates
- **Confounders**: Macroeconomic controls
- **Result**: Unbiased average treatment effect estimation

### 4. **Causal Forest**

- **Model**: CausalForestDML for heterogeneous effects
- **Features**: Economic conditions, firm characteristics, policy timing
- **Analysis**: Individual treatment effect estimation
- **Insights**: Policy effectiveness varies by economic regime

### 5. **Hybrid Ensemble**

- **Weighting**: Performance-based ensemble weights
- **Integration**: Weighted combination of LSTM, DML, and Causal Forest
- **Scenarios**: Multiple policy simulation frameworks
- **Uncertainty**: Multi-source uncertainty quantification

## ğŸ“Š Key Results

### Model Performance

- **LSTM**: Captures temporal dependencies in economic data
- **Double ML**: Estimates average treatment effect with confidence intervals
- **Causal Forest**: Discovers heterogeneous policy impacts (98.5% ensemble weight)
- **Hybrid**: Robust policy recommendations with uncertainty quantification

### Policy Insights

- **Tax Cuts**: Generally positive impact on firm survival
- **Economic Regime**: Policy effectiveness varies by economic conditions
- **Heterogeneity**: Effects differ across firm sizes and economic stress levels
- **Forecasting**: 5-year projections with scenario analysis

## ğŸ”¬ Academic Contributions

### Methodological Innovation

1. **Multi-Model Integration**: Novel combination of forecasting and causal inference
2. **Real Data Validation**: Comprehensive analysis using only verified economic data
3. **Ensemble Methodology**: Data-driven weighting scheme for robust predictions
4. **Publication Standards**: Academic-quality outputs and reproducible research

### Policy Relevance

1. **Evidence-Based**: Quantitative assessment of tax policy effectiveness
2. **Heterogeneity**: Recognition that policy impacts vary across contexts
3. **Uncertainty**: Honest reporting of prediction confidence intervals
4. **Practical**: Actionable recommendations for policymakers

## ğŸ“ˆ Usage Examples

### Running Individual Components

```python
# Initialize data pipeline
data_pipeline = EconomicDataPipeline()
master_data = data_pipeline.create_master_dataset()

# Train LSTM forecaster
lstm_forecaster = LSTMForecaster()
lstm_model = lstm_forecaster.train_lstm_model(master_data)

# Run causal analysis
dml_analyzer = DoubleMachineLearning()
causal_effects = dml_analyzer.estimate_treatment_effects(master_data)

# Generate hybrid forecasts
hybrid_analyzer = HybridEconomicAnalyzer(lstm_forecaster, dml_analyzer, causal_forest_analyzer)
policy_scenarios = hybrid_analyzer.simulate_policy_scenarios(master_data)
```

### Export Results

```python
# Create publication package
pub_exporter = PublicationExporter(data_pipeline, lstm_forecaster, dml_analyzer, causal_forest_analyzer, hybrid_analyzer)
results_package = pub_exporter.export_all_results()
```

## ï¿½ Expected Output Formats

The hybrid analysis framework produces three main types of quantitative outputs:

### 1. Policy Impact Quantification

The framework provides detailed quantitative assessment of policy impacts with confidence intervals:

```json
{
  "tax_cut_2%": {
    "causal_effect_on_survival": 0.0245, // 2.45% increase in survival
    "confidence_interval": [0.0089, 0.0401],
    "affected_firms": 18500,
    "economic_conditions": "growth_responsive"
  },
  "tax_increase_3%": {
    "causal_effect_on_survival": -0.0189, // 1.89% decrease in survival
    "confidence_interval": [-0.0356, -0.0022],
    "affected_firms": 12300,
    "economic_conditions": "recession_sensitive"
  },
  "aggressive_tax_cut_5%": {
    "causal_effect_on_survival": 0.0512, // 5.12% increase in survival
    "confidence_interval": [0.0234, 0.079],
    "affected_firms": 25000,
    "economic_conditions": "universally_positive"
  }
}
```

### 2. Comparative Model Performance

Comprehensive comparison across all methodologies with validation metrics:

```
Model             | RMSE  | RÂ²    | Causal Validity | Ensemble Weight
------------------|-------|-------|-----------------|----------------
LSTM Forecast     | 0.0342| 0.863 | N/A            | 0.010 (1.0%)
Double ML         | 0.0456| 0.794 | High           | 0.048 (4.8%)
Causal Forest     | 0.0298| 0.881 | High           | 0.985 (98.5%)
Hybrid Ensemble   | 0.0287| 0.895 | High           | 1.000 (Combined)

Performance Notes:
- Causal Forest dominates ensemble due to superior heterogeneity detection
- LSTM provides temporal forecasting foundation
- DML ensures unbiased causal estimation
- Hybrid achieves best overall predictive performance
```

### 3. Policy Scenario Analysis

Detailed scenario-specific analysis with actionable recommendations:

```
Scenario: Moderate Tax Cut (2%) during Economic Expansion
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ LSTM Base Forecast:     91.7% firm survival rate
â€¢ Causal Effect (DML):    +2.45% survival improvement
â€¢ Heterogeneous Effect:   +2.58% (Causal Forest)
â€¢ Hybrid Prediction:      94.2% adjusted survival rate
â€¢ Confidence Interval:    [92.8%, 95.6%]
â€¢ Policy Recommendation: STRONGLY POSITIVE - Implement immediately

Scenario: Tax Increase (3%) during Economic Downturn
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ LSTM Base Forecast:     87.4% firm survival rate
â€¢ Causal Effect (DML):    -1.89% survival reduction
â€¢ Heterogeneous Effect:   -2.12% (Causal Forest)
â€¢ Hybrid Prediction:      85.3% adjusted survival rate
â€¢ Confidence Interval:    [83.7%, 86.9%]
â€¢ Policy Recommendation: HIGH RISK - Delay until recovery

Scenario: Aggressive Tax Cut (5%) during Stable Growth
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ LSTM Base Forecast:     90.1% firm survival rate
â€¢ Causal Effect (DML):    +5.12% survival improvement
â€¢ Heterogeneous Effect:   +4.89% (Causal Forest)
â€¢ Hybrid Prediction:      95.0% adjusted survival rate
â€¢ Confidence Interval:    [93.3%, 96.7%]
â€¢ Policy Recommendation: OPTIMAL TIMING - Maximum positive impact
```

### Output File Structure

All results are automatically exported to structured files:

```
exports/
â”œâ”€â”€ table3_policy_impact_summary.csv     # Quantified policy impacts
â”œâ”€â”€ table2_model_performance.csv         # Model comparison metrics
â”œâ”€â”€ table4_economic_forecasts.csv        # Scenario predictions
â”œâ”€â”€ executive_summary.json               # Complete results summary
â””â”€â”€ results_package_manifest.json        # Export metadata
```

## ï¿½ğŸ“ Data Sources & Verification

### Primary Data Sources

- **Federal Reserve Economic Data (FRED)**: GDP, CPI, unemployment, interest rates
- **Bureau of Labor Statistics (BLS)**: Employment and labor force statistics
- **Business Dynamics Statistics (BDS)**: Firm creation, destruction, and survival rates

### Data Verification

âœ… **All data sources verified as real economic data**  
âŒ **No synthetic or artificial data used anywhere in analysis**  
âœ… **All data collection methods documented and reproducible**  
âœ… **API keys and data access properly managed**

## ğŸ”„ Reproducibility

### Complete Reproducibility

- **Random Seeds**: All set to 42 across all models
- **Version Control**: Complete Git history available
- **Environment**: Dependency versions locked
- **Documentation**: Comprehensive code comments and markdown

### Validation

- **Cross-Validation**: Time series appropriate validation methods
- **Robustness**: Multiple model specifications tested
- **Sensitivity**: Parameter sensitivity analysis conducted
- **Peer Review**: Code available for academic review

## ğŸ“š References & Citations

### Methodological Papers

- Chernozhukov, V., et al. (2018). "Double/debiased machine learning for treatment and structural parameters"
- Wager, S., & Athey, S. (2018). "Estimation and inference of heterogeneous treatment effects using random forests"
- Hochreiter, S., & Schmidhuber, J. (1997). "Long short-term memory"

### Data Sources

- Federal Reserve Bank of St. Louis. Federal Reserve Economic Data (FRED)
- U.S. Bureau of Labor Statistics. Labor Force Statistics
- U.S. Census Bureau. Business Dynamics Statistics

## ğŸ‘¥ Contributing

This is an academic research project. For questions or collaboration inquiries, please contact the research team.

### Development Guidelines

1. **Data Integrity**: Maintain exclusive use of real economic data
2. **Reproducibility**: Ensure all changes maintain reproducibility
3. **Documentation**: Update documentation for any methodology changes
4. **Testing**: Validate changes against known results

## ğŸ“„ License

This project is developed for academic research purposes. Please cite appropriately if using this methodology in your research.

## ğŸ”— Contact

For questions about this research or potential collaborations, please reach out through the repository's issue tracker.

---

**Note**: This project represents cutting-edge research in economic policy analysis. All findings should be interpreted within the context of the methodological assumptions and data limitations discussed in the full academic paper.

## ğŸ—‚ï¸ Unified Thesis Asset Management

All figures, tables, models, summaries, and data snapshots used in the thesis are now consolidated under a single structured directory: `thesis_images/` (created automatically).

### Directory Layout

```
thesis_images/
â”œâ”€â”€ figures/          # All static visual assets (PNG/SVG/PDF)
â”œâ”€â”€ tables/           # CSV tables (plus optional JSON metadata)
â”œâ”€â”€ data_snapshots/   # Intermediate / snapshot datasets
â”œâ”€â”€ models/           # Serialized model artifacts
â”œâ”€â”€ summaries/        # Executive narrative or JSON summaries
â””â”€â”€ manifests/        # Auto-generated manifest (JSON + CSV)
```

### Naming Convention

Each asset follows:

```
<section>__<short_descriptor>__v<major.minor>.<ext>
```

Examples:

```
forecasting__lstm_training_history__v1.0.png
policy_analysis__policy_impact_quantification__v1.0.csv
causal_inference__causal_forest_feature_importance__v1.0.csv
```

Sections currently used (heuristic mapping):

- `forecasting`
- `causal_inference`
- `policy_analysis`
- `model_evaluation`
- `general`

### Manifest

The file `thesis_images/manifests/asset_manifest.json` (and CSV) contains structured metadata for every registered asset:

Fields:

```
id, category, section, name, version, filename, path, created_at, tags, description, source_code_reference
```

You can load it in Python:

```python
import json, pandas as pd
with open('thesis_images/manifests/asset_manifest.json') as f:
    data = json.load(f)
manifest_df = pd.DataFrame(data)
manifest_df.head()
```

### Programmatic Saving

Use the `ThesisAssetManager` to save future assets directly:

```python
from thesis_asset_manager import ThesisAssetManager
assets = ThesisAssetManager().ensure()

# After creating a matplotlib plot:
fig, ax = plt.subplots()
ax.plot([1,2,3],[2,3,4])
assets.save_figure(fig, section='forecasting', name='example_trend', description='Demo figure', tags=['demo','forecasting'])

# Saving a table (pandas DataFrame):
assets.save_table(df, section='model_evaluation', name='new_performance_metrics', version='1.1', description='Updated metrics')

# Saving a textual or JSON summary:
assets.save_summary({'result': 'positive', 'ate': 0.024}, section='policy_analysis', name='tax_cut_summary')
```

### Retroactive Organization

Existing artifacts were imported using `organize_thesis_assets.py`. Re-run if new legacy exports are added:

```bash
python organize_thesis_assets.py
```

### Updating an Asset

Increment the version (e.g. `v1.1`) when a figure or table is regenerated with meaningful changes. The manifest automatically deduplicates the `(category, section, name, version)` combination.

### Referencing in Thesis Text

Because each filename embeds its semantic meaning, you can directly cite:

- Figure: `forecasting__hybrid_model_results__v1.0.png`
- Table: `policy_analysis__policy_impact_quantification__v1.0.csv`

This ensures stable, unambiguous references in manuscript drafts.

### Benefits

- Centralized, versioned, and searchable assets
- Stable naming for citation and LaTeX inclusion
- Automated manifest for cross-referencing and reproducibility
- Extensible: add new categories or metadata fields via `thesis_asset_manager.py`

### Future Enhancements (Ideas)

- Automatic LaTeX table & figure snippet generation
- Hash-based change detection to auto-bump versions
- Inline notebook widget to browse and insert asset references

---

For any new analysis step, prefer using the asset manager methods instead of ad-hoc `plt.savefig` or `DataFrame.to_csv` calls to maintain consistency.
